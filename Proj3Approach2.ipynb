{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Amazon Review Clustering approach 2\n",
        "\n"
      ],
      "metadata": {
        "id": "8KKJdcYMIppn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NWVji6dRYz7k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import tarfile\n",
        "import json\n",
        "import seaborn as sns  \n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "import pickle\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TWfusN8HVmjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce1b36e-b3d7-4296-b96f-e584699b78a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "path = Path('/content/drive/MyDrive/Colab Notebooks/data_1pct.json')\n",
        "\n",
        "print(path.is_file())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hlYq5rtiVabk"
      },
      "outputs": [],
      "source": [
        "df_data = pd.read_json('/content/drive/MyDrive/Colab Notebooks/data_1pct.json', lines=True, orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOZl-07z__Rh"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OY6VPyKQZEMK"
      },
      "outputs": [],
      "source": [
        "df_new = df_data.loc[:, ['reviewText', 'summary']]\n",
        "df_new['reviewText'] = df_new['reviewText'].replace(np.nan, '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnH0uvXrZEvK",
        "outputId": "6d72636a-86da-40b1-b92b-cdd3ac721f20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vO1UhyleZWKy",
        "outputId": "d708b590-4004-47d4-b2a2-9264d22411b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download required NLTK resources\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Tokenize text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    # Tokenize sentences into words and tag parts of speech\n",
        "    tagged_words = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        tagged = nltk.pos_tag(words)\n",
        "        tagged_words.extend(tagged)\n",
        "    # Extract noun phrases\n",
        "    chunks = nltk.ne_chunk(tagged_words, binary=False)\n",
        "    phrases = []\n",
        "    for chunk in chunks:\n",
        "        if hasattr(chunk, 'label') and chunk.label() == 'NP':\n",
        "            phrases.append(' '.join([c[0] for c in chunk]))\n",
        "    # Remove stop words and lemmatize words\n",
        "    words = [lemmatizer.lemmatize(word.lower()) for word in phrases if word.lower() not in stop_words and len(word) > 2]\n",
        "    # Join words back into text\n",
        "    text = ' '.join(words)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "84hju83o_AlJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to 'reviewText' and 'summary' columns\n",
        "df_new['reviewText'] = df_new['reviewText'].apply(preprocess_text)\n",
        "df_new['summary'] = df_new['summary'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "r3VY8FPw_ENM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Applying LDA and calculating Coherence"
      ],
      "metadata": {
        "id": "dSNKNbu8_mTo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs5Jli7qSrow"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models import CoherenceModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6br-Hh0HRpUV"
      },
      "outputs": [],
      "source": [
        "# Convert the preprocessed text to a list of words\n",
        "docs = df_new['reviewText'].apply(lambda x: x.split())\n",
        "\n",
        "# Create a dictionary of the words in the corpus\n",
        "dictionary = Dictionary(docs)\n",
        "\n",
        "# Filter out words that appear in less than 10 documents or more than 50% of the documents\n",
        "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
        "\n",
        "# Convert the corpus to a bag of words format\n",
        "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "\n",
        "# Choose the number of topics for the LDA model\n",
        "num_topics = 10\n",
        "\n",
        "# Fit the LDA model to the corpus\n",
        "lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation"
      ],
      "metadata": {
        "id": "Fo05nlRRIW16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model's performance using coherence score\n",
        "coherence_model_lda = CoherenceModel(model=lda, texts=docs, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score: ', coherence_lda)\n",
        "\n",
        "# Inspect the topics\n",
        "topics = lda.show_topics(num_topics=num_topics, formatted=False)\n",
        "for i, topic in enumerate(topics):\n",
        "    print('Topic {}: {}'.format(i, [word[0] for word in topic[1]]))"
      ],
      "metadata": {
        "id": "pUyFnHulIYec"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}