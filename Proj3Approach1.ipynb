{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWZr-FCafjYq"
      },
      "source": [
        "# Amazon Review Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCk7Bbt16wOD"
      },
      "source": [
        "import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DMowlj7Pfb5i"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import seaborn as sns  \n",
        "import matplotlib.pyplot as plt \n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import pickle\n",
        "import sklearn\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jiemw6v0603_"
      },
      "source": [
        "Load the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "e7EZQOI9edqo"
      },
      "outputs": [],
      "source": [
        "df_data = pd.read_json('/content/drive/MyDrive/Colab Notebooks/data_1pct.json', lines=True, orient='records')\n",
        "\n",
        "#Removing the unwanted columns\n",
        "df_new = df_data.loc[:, ['reviewText', 'summary']]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --user -U nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX4VjpFNp9Lp",
        "outputId": "8d946af4-9474-47a1-f100-36dbf22007dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.8.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2uKQ_K9YJ34",
        "outputId": "86ab1977-32ec-45d7-ee1d-9d0a7ce6a46a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The nltk version is 3.8.1.\n",
            "The scikit-learn version is 1.2.1.\n"
          ]
        }
      ],
      "source": [
        "print('The nltk version is {}.'.format(nltk.__version__))\n",
        "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8adxT-GQnkm"
      },
      "source": [
        "Download necessary NLTK resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6-t9gfKi4MN",
        "outputId": "ddb438f1-e9eb-4c53-9717-a3347f569210"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9q_vKwi1BVGb"
      },
      "outputs": [],
      "source": [
        "# Define stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define stemmer\n",
        "stemmer = PorterStemmer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgbripSYQuwE"
      },
      "source": [
        "Preprocessing the Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "leWEcvPJBYHr"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert input to string\n",
        "    text = str(text)\n",
        "    # Tokenize text into words\n",
        "    words = word_tokenize(text.lower())\n",
        "    # Remove stop words and words with length <= 2\n",
        "    words = [word for word in words if word not in stop_words and len(word) > 2]\n",
        "    # Stem words\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    # Join words back into text\n",
        "    text = ' '.join(words)\n",
        "    return text\n",
        "# Apply preprocessing to 'reviewText' and 'summary' columns\n",
        "df_new['reviewText'] = df_new['reviewText'].apply(preprocess_text)\n",
        "df_new['summary'] = df_new['summary'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J27UyQVsQ6pL"
      },
      "source": [
        "Choose and fit the LDA model to the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cs5Jli7qSrow"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.models import CoherenceModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alRpE6hWStWg",
        "outputId": "5fffc99d-9850-4f16-bed9-0aab21877e9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coherence Score:  0.4588494582321574\n",
            "Topic 0: ['...', 'open', '....', 'oven', 'manual', 'class=', 'reliabl', 'function', '.....', 'ie=utf8']\n",
            "Topic 1: ['set', 'excel', 'product', 'look', 'perk', 'qualiti', 'order', 'color', 'inch', 'box']\n",
            "Topic 2: ['knife', 'use', 'handl', 'cut', 'stainless', 'steel', 'well', \"n't\", 'like', 'blade']\n",
            "Topic 3: [\"n't\", 'filter', 'get', 'would', 'like', 'use', 'time', 'review', 'work', 'thing']\n",
            "Topic 4: ['pan', 'egg', 'use', 'cook', 'make', 'cake', \"n't\", 'food', 'heat', 'time']\n",
            "Topic 5: ['love', 'use', 'easi', 'great', 'perfect', 'clean', 'one', 'make', 'recommend', 'size']\n",
            "Topic 6: ['one', 'year', 'use', 'replac', \"n't\", 'bought', 'last', 'time', 'purchas', 'buy']\n",
            "Topic 7: ['great', 'work', 'good', 'well', 'price', 'product', 'qualiti', 'nice', 'expect', 'made']\n",
            "Topic 8: ['best', 'ever', 'electr', \"'ve\", 'kitchen', 'poach', 'thing', 'awesom', 'amaz', 'make']\n",
            "Topic 9: ['coffe', 'cup', 'make', 'water', 'maker', 'pot', 'brew', 'hot', 'mug', 'use']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "docs = df_new['reviewText'].apply(lambda x: x.split())\n",
        "\n",
        "# Create a dictionary of the words in the corpus\n",
        "dictionary = Dictionary(docs)\n",
        "\n",
        "# Filter out words that appear in less than 10 documents or more than 50% of the documents\n",
        "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
        "\n",
        "# Convert the corpus to a bag of words format\n",
        "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
        "\n",
        "# Choose the number of topics for the LDA model\n",
        "num_topics = 10\n",
        "\n",
        "# Fit the LDA model to the corpus\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, passes=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation"
      ],
      "metadata": {
        "id": "d1mGDIYbIb-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model's performance using coherence score\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('Coherence Score: ', coherence_lda)\n",
        "\n",
        "# Inspect the topics\n",
        "topics = lda_model.show_topics(num_topics=num_topics, formatted=False)\n",
        "for i, topic in enumerate(topics):\n",
        "    print('Topic {}: {}'.format(i, [word[0] for word in topic[1]]))"
      ],
      "metadata": {
        "id": "azlnuyuWIhJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtDnJwXhW1Sg"
      },
      "source": [
        "Write the mode in the pickel file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GuP0FJ7DW9ZG"
      },
      "outputs": [],
      "source": [
        "with open('lda_model.pkl', 'wb') as f:\n",
        "    pickle.dump(lda_model, f)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ieFbk9SkXhA3"
      },
      "outputs": [],
      "source": [
        "with open('lda_model.pkl', 'rb') as f:\n",
        "    lda_model = pickle.load(f)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}